<!DOCTYPE html>
<html lang="en-US" dir="ltr">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!--  
    Document Title
    =============================================
  -->
  <title>UG2+ Challenge</title>
    <!--  
    Favicons
    =============================================
  -->
  <link rel="apple-touch-icon" sizes="57x57" href="assets/images/favicons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="assets/images/favicons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="assets/images/favicons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="assets/images/favicons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="assets/images/favicons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="assets/images/favicons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="assets/images/favicons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="assets/images/favicons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="assets/images/favicons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192" href="assets/images/favicons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="assets/images/favicons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="assets/images/favicons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="assets/images/favicons/favicon-16x16.png">
  <link rel="manifest" href="/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="assets/images/favicons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
    <!--  
    Stylesheets
    =============================================
    
  -->
  <!-- Default stylesheets-->
  <link href="assets/lib/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet">
  <!-- Template specific stylesheets-->
  <link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:400,700" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Volkhov:400i" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700,800" rel="stylesheet">
  <link href="assets/lib/animate.css/animate.css" rel="stylesheet">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
  <link href="assets/lib/et-line-font/et-line-font.css" rel="stylesheet">
  <link href="assets/lib/flexslider/flexslider.css" rel="stylesheet">
  <link href="assets/lib/owl.carousel/dist/assets/owl.carousel.min.css" rel="stylesheet">
  <link href="assets/lib/owl.carousel/dist/assets/owl.theme.default.min.css" rel="stylesheet">
  <link href="assets/lib/magnific-popup/dist/magnific-popup.css" rel="stylesheet">
  <link href="assets/lib/simple-text-rotator/simpletextrotator.css" rel="stylesheet">
  <!-- Main stylesheet and color file-->
  <link href="assets/css/style.css" rel="stylesheet">
  <link id="color-scheme" href="assets/css/colors/default.css" rel="stylesheet">
</head>
<body data-spy="scroll" data-target=".onpage-navigation" data-offset="60">
  <a id="ddmenuLink" href="menu_transparent.html">Menu</a>
  <main>
    <div class="page-loader">
      <div class="loader">Loading...</div>
    </div>   

    <div class="main">
      <section class="module bg-dark-30 portfolio-page-header" data-background="assets/images/rulesbg.png" style="padding: 30px 0;">
        <div class="container">
          <<div class="row" style="padding-top: 40px">
            <div class="col-sm-12">
              <h2 class="module-title font-alt" style="margin: 0 0 20px">Guidelines</h2>
              <h3 class="module-subtitle font-serif" style="margin: 0 0 20px"><a href="#timetable" class="section-scroll btn btn-border-w btn-round">Time table</a>  <a href="#prizes" class="section-scroll btn btn-border-w btn-round">Prizes</a> <a href="#submission" class="section-scroll btn btn-border-w btn-round">Submission process</a> <a href="#rules" class="section-scroll btn btn-border-w btn-round">Rules</a> <a href="#framework" class="section-scroll btn btn-border-w btn-round">Framework</a><a href="#FAQ" class="section-scroll btn btn-border-w btn-round">FAQ</a></h3>
            </div>
          </div>
        </div>
      </section>

      <div class="container" style="font-size: 30px; padding-top: 40px; padding-bottom: 20px">
          Link to the final submission form was sent via email. If you did not receive it, please email cvpr2020.ug2challenge.track2@gmail.com
        </div>

      <section class="module-medium" style="padding:20px 0px 0px" id='timetable'>
        <div class="container">
          <div class="row" style="text-align: justify">
            <div class="col-sm-12 font-nat" style="font-size: 13pt">            
              <h3 class="module-title font-alt" style="margin: 0px; padding: 30px 0 20px">Timetable</h3>
              <ul>
                <li>January 31, 2020: Development kit and registration made available</li>
                <li>March 15, 2020: Registration deadline</li>
                <li>April 1-8, 2020: Dry run period**</li>
                <li>April 8, 2020: Submissions deadline</li>
              </ul>
              ** Every contestant will have the opportunity to verify before the submission window closes that their Docker container submission works. This dry run is completely optional but is highly recommended.

            </div>
          </div>
        </div>
      </section>

      <section class="module-medium" style="padding:20px 0px 0px" id='prizes'>
        <div class="container">
          <div class="row" style="text-align: justify">
            <div class="col-sm-12 font-nat" style="font-size: 13pt">            
              <h3 class="module-title font-alt" style="margin: 0px; padding: 30px 0 20px">Prizes</h3>
              <p>UG<sup>2</sup>+ evaluates algorithms for FlatCam image enhancement, reconstruction, and face verification. The most successful and innovative teams will be invited to present at the CVPR 2020 workshop. We provide three sub-challenge categories:</p>
              <ol>
                <li>
                  FlatCam for Faces: Enhancement, Reconstruction, and Verification
                  <ol>
                    <li>
                      Image Enhancement for FlatCam Face Verification
                      <ul>
                        <li>1st Place: $500</li>
                        <li>2nd Place: $300</li>
                        <li>3rd Place: $200</li>
                      </ul>
                    </li>
                    <li>
                      Image Reconstruction for FlatCam Face Verification
                      <ul>
                        <li>1st Place: $500</li>
                        <li>2nd Place: $300</li>
                        <li>3rd Place: $200</li>
                      </ul>
                    </li>
                    <li>
                      End-to-End Verification on FlatCam Measurements
                      <ul>
                        <li>1st Place: $500</li>
                        <li>2nd Place: $300</li>
                        <li>3rd Place: $200</li>
                      </ul>
                    </li>
                  </ol>
                </li>
                
              </ol>
              For a total of $???K awarded in prizes.

            </div>
          </div>
        </div>
      </section>

      <section class="module-medium" style="padding:20px 0px 0px" id='subchallenges'>
        <div class="container">
          <div class="row" style="text-align: justify">
            <div class="col-sm-12 font-nat" style="font-size: 13pt">            
              <h3 class="module-title font-alt" style="margin: 0px; padding: 30px 0 20px">Background Information</h3>
              <p style="text-align: justify;">In this challenge, you will explore performing face verification using FlatCam images. 
                In face verification, an algorithm takes as input two images, each containing a face, and must then output whether the images contain the same identity or not.
                Face verification has been an active area of research with many algorithms that currently exist.
                While most of these off-the-shelf algorithms perform with very high accuracy for face images captured with standard lens-based cameras, they do not perform very well for FlatCam images.
                The FlatCam is a lensless camera whose measurements do not resemble an image. An image reconstruction algorithm based on Tikhonov regularization (detailed <a href="https://arxiv.org/pdf/1509.00116.pdf">here</a>) maps the measurements into a recognizable image.
                The resulting reconstruction, which we call the <b>Tikhonov reconstruction</b>, contains unique artifacts and noise, which may be the reason of decreased face verification accuracy.
              </p>
              <p style="text-align: justify;">There are three possible approaches to deal with this issue.
                First, one can enhance the Tikhonov reconstruction to output images that better match those produced by standard lens-based cameras.
                Second, one can design better reconstruction algorithms for the FlatCam measurements.
                Lastly, one can design new verification algorithms that can perform better for FlatCam images.
                These approaches are explored in the following three subchallenges:</p>
              <ol style="text-align: justify;">
                <li><b>Sub-Challenge 2.1: Image Enhancement for FlatCam Face Verification.</b> 
                  You will be given Tikhonov reconstructed FlatCam images, and you must design an algorithm that enhances these images to improve the performance of face verification algorithms performed on them. 
                  To gauge the effectiveness of your algorithm, 3 off-the-shelf face verification algorithms will be applied to Tikhonov reconstructed FlatCam images from a hold-out test set pre-processed by your enhancement algorithm. 
                  Your score will be the average accuracy rate of the face verification algorithms.
                  Note that you will not design a face verification algorithm or a reconstruction algorithm.
                </li>
                <li><b>Sub-Challenge 2.2: Image Reconstruction for FlatCam Face Verification.</b> 
                  You will design a new reconstruction algorithm that maps raw FlatCam sensor measurements into face images that are then input to a face verification algorithm. 
                  You will be given calibration information for the FlatCam prototype used, as well as code for the standard reconstruction algorithm, which you may or may not use. 
                  You will not design a novel face verification algorithm. 
                  For testing, your algorithm would be applied to the sensor measurements of a hold-out test set whose outputs are then passed into 3-4 off-the-shelf face verification algorithms.
                  Your score will be the average accuracy rate of the face verification algorithms.
                </li>
                <li><b>Sub-Challenge 2.3: End-to-End Face Verification on FlatCam Measurements.</b>
                  You will design an end-to-end procedure for performing face verification directly on FlatCam sensor measurements. 
                  Your should take in a pair of sensor measurements and output whether the two images contain the same identity or not. 
                  No image reconstruction nor enhancement component is necessary in the final pipeline, although they are also not forbidden.
                  Your score will be the accuracy rate of your predictions (number of correct predictions divided by total image pairs).
                </li>
            </div>
          </div>
        </div>
      </section>

      <section class="module-medium" style="padding:20px 0px 0px" id='submission'>
        <div class="container">
          <div class="row" style="text-align: justify">
            <div class="col-sm-12 font-nat" style="font-size: 13pt">            
              <h3 class="module-title font-alt" style="margin: 0px; padding: 30px 0 20px">Submission Process</h3>
              <p style="text-align: justify;">To submit your results to our leaderboard, follow these steps:</p>
              <ol style="text-align: justify;">
                <li>Ensure that your team has registered using this registration form: <span class="font-serif"><a href="https://forms.gle/hQueBuMmfDx182EH9">LINK</a></span></li>
                <li>Download available training data from: <a style="color: #337ab7; text-decoration: underline" href="http://cvpr2020.ug2challenge.org/dataset20_t2.html">Training Data</a>.</li>
                <li>Create a standalone Docker image that contains your models and code for running your algorithm following these requirements:
                  <ul>
                    <li><b>Sub-challenge 2.1:</b> Your Docker image must contain the file "/root/ug2_challenge2-1_submission#.sh" (# = 1, 2, or 3 and refers to the number of submission).
                      When run, this script must enhance all the images (png format) in the directory "/root/challenge2-1_test_input" and save the outputs in "/root/challenge2-1_test_output/submission#", keeping all the image names the same.
                      In test time, we will mount the folder "/root/challenge2-1_test_input" containing the reconstructed FlatCam images of the hold-out test set from which your algorithm will be evaluated.
                      We will also mount a file "/root/test_filenames.txt" that will contain the filename of each image in "/root/challenge2-1_test_input" that you may use.
                      See the Devkit for examples.
                    </li>
                    <li><b>Sub-challenge 2.2:</b> Your Docker image must contain the file "/root/ug2_challenge2-2_submission#.sh" (# = 1, 2, or 3 and refers to the number of submission).
                        When run, this script must reconstruct all the FlatCam measurements (png format) in the directory "/root/challenge2-2_test_input" and save the outputs in "/root/challenge2-2_test_output/submission#", keeping all the image names the same.
                        In test time, we will mount the folder "/root/challenge2-2_test_input" containing the FlatCam measurements of the hold-out test set from which your algorithm will be evaluated.
                        We will also mount a file "/root/test_filenames.txt" that will contain the filename of each image in "/root/challenge2-2_test_input" that you may use.
                        A sample Docker image is provided with the DevKit as a guide.
                    </li>
                    <li><b>Sub-challenge 2.3:</b> Your Docker image must contain the file "/root/ug2_challenge2-3_submission#.sh" (# = 1, 2, or 3 and refers to the number of submission).
                        In test time, we will mount the following files:
                        <ul>
                          <li>/root/challenge2-3_test_pairs.txt: a text file where each line contains a space-separated pair of filenames of FlatCam measurements (absolute path).</li>
                          <li>/root/challenge2-3_test_input: a directory containing the images referred to in /root/challenge2-3_test_pairs.txt</li>
                        </ul>
                        Your submission script, when run, must output a text file "/root/ug2_challenge2-3_test_output_submission#.txt" with the same number of lines as "/root/challenge2-3_test_pairs.txt" wherein each line is either 1 or 0 pertaining to whether your algorithm predicts that the pair of images in the text file contains the same identity or not, respectively.
                    </li>
                  </ul>
                </li>
                <li>Upload your Docker image to <a href="https://hub.docker.com/">Docker Hub</a> and submit your docker link to the corresponding sub challenge. Check your uploaded docker with the testing code provided in our DevKits to make sure that the uploaded docker can work correctly. Ensure that your code can be run using the hardware listed under the Requirements section.</li>
                <li>Each team may submit up to 3 algorithms per sub challenge. All submissions (per sub challenge) will be held within one single Docker image to be uploaded to Docker Hub. The Docker image will contain all dependencies and code required to perform the model’s operation and will execute the model(s) contained upon run.</li>
              </ol>
              <h3 class="work-details-title font-alt">Development Kit</h3>
              <p style="text-align: justify;">
                The Development Kit (<a href="https://github.com/tanjasper/ug2_2020_challenge2_devkit">LINK</a>) contains three components:
                <ul>
                  <li>Python and Matlab codes for standard FlatCam reconstruction as detailed <a href="https://arxiv.org/pdf/1509.00116.pdf">here</a></li>
                  <li>Sample evaluation script with sample test images. 
                    This will only check that your code runs properly and will not do the actual evaluation during test.
                    The sample test images provided are randomly drawn from the training sets and do not contain any evaluation images that will be used in the actual test time.
                    </li>
                  <li>A sample Docker image with correctly formatted submissions. Your Docker image need not inherit from this one.</li>
                </ul>
              </p>
              <p style="text-align: justify;">Each team must submit one algorithm for each challenge they wish to participate in. Participants who have investigated several algorithms may submit up to 3 algorithms per subchallenge. All submissions (per subchallenge) will be held within one single Docker container to be uploaded to Docker Hub. The Docker container will container all dependencies and code required to perform the model’s operation and will execute the model(s) contained upon run.</p>
              <p style="text-align: justify;">The input images will be provided to the container at run time through Docker’s mounting option, as will the output folders for the model(s) to save their results.</p>
              <h3 class="work-details-title font-alt">Requirements</h3>
              <b>Software</b>
              <ul>
                <li>Docker-CE</li>
                <li>NVIDIA Docker</li>
              </ul>
              <b>Hardware</b>
              <p style="text-align: justify;">The proposed algorithms should be able to run in systems with:</p>
              <ul>
                <li>Up to and including RTX 2080 Ti 11 GB</li>
                <li>Up to and including 12 cores</li>
                <li>Up to and including 32gb memory</li>
              </ul>
              <p style="text-align: justify;">If you have any questions about this challenge track please feel free to email <a style="color: #337ab7; text-decoration: underline" href="mailto:cvpr2020.ug2challenge.track2@gmail.com">cvpr2020.ug2challenge.track2@gmail.com</a></p>
            </div>
          </div>
        </div>
      </section>

      <section class="module-medium" style="padding:20px 0px 0px" id='rules'>
        <div class="container">
          <div class="row" style="text-align: justify">
            <div class="col-sm-12 font-nat" style="font-size: 13pt">            
              <h3 class="module-title font-alt" style="margin: 0px; padding: 30px 0 20px">Rules</h3>
              <p>Read carefully the following guidelines before submitting. Methods not complying with the guidelines will be disqualified.</p>
              <ul>
                <li>We encourage participants to use the provided training and validation data for each task, as well as to make use of their own data or data from other sources for training. However the use any form of annotation or use of any of the provided benchmarks test sets for either supervised or unsupervised training is strictly forbidden.</li>

                <li>Participants who have investigated several algorithms may submit up to 3 algorithms per challenge. Only a single submission per participant can be the winner of a single sub-challenge. Changes in algorithm parameters do not constitute a different method, all parameter tuning must be conducted using the dataset provided and any additional data the participants consider appropriate.</li>

                <li>
                  If an algorithm is submitted that makes inefficient use of deep learning, we may not be able to process the entire test set due to time constraints. To avoid this possibility, we will place limits on per-image runtime.
                </li>
                <li>
                  Results can vary based on the GPU used. We noticed that results varied slightly across GPU models. We are restricting our evaluation to one model (RTX 2080 Ti) and will encourage participants to use this model in their development.
                </li>
              </ul>
              <h3 class="work-details-title font-alt">Eligibility</h3>
              <ul>
                <li><p style="text-align: justify;">Foreign Nationals and International Developers:  All Developers can participate with this exception: residents of, Iran, Cuba, North Korea, Crimea Region of Ukraine, Sudan or Syria or other countries prohibited on the U.S. State Department’s State Sponsors of Terrorism list.  In addition, Developers are not eligible to participate if they are on the Specially Designated National list promulgated and amended, from time to time, by the United States Department of the Treasury.  It is the responsibility of the Developer to ensure that they are allowed to export their technology solution to the United States for the Live Test.  Additionally, it is the responsibility of participants to ensure that no US law export control restrictions would prevent them from participating when foreign nationals are involved.  If there are US export control concerns, please contact the organizers and we will attempt to make reasonable accommodations if possible.</p></li>
                <li><p style="text-align: justify;">If you are entering as a representative of a company, educational institution or other legal entity, or on behalf of your employer, these rules are binding on you, individually, and/or the entity you represent or are an employee. If you are acting within the scope of your employment, as an employee, contractor, or agent of another party, you warrant that such party has full knowledge of your actions and has consented thereto, including your potential receipt of a prize. You further warrant that your actions do not violate your employer’s or entity’s policies and procedures.</p></li>
                <li><p style="text-align: justify;">The organizers reserve the right to verify eligibility and to adjudicate on any dispute at any time. If you provide any false information relating to the prize challenge concerning your identity, email address, ownership of right, or information required for entering the prize challenge, you may be immediately disqualified from the challenge.</p></li>
                <li><p style="text-align: justify;">Individual Account. You may make submissions only under one, unique registration. You will be disqualified if you make submissions through more than one registration. You may submit up to 3 submissions (one per challenge), containing at most 3 algorithms per submission. Any submissions that does not adhere to this will be disqualified.</p></li>
              </ul>
              <p>The organizers reserve the right to disqualify any participating team for any of the reasons mentioned above and if deemed necessary.</p>
              <h3 class="work-details-title font-alt">Warranty, indemnity and release</h3>
              <p>You warrant that your Submission is your own original work and, as such, you are the sole and exclusive owner and rights holder of the Submission, and you have the right to make the Submission and grant all required licenses. You agree not to make any Submission that: (i) infringes any third party proprietary rights, intellectual property rights, industrial property rights, personal or moral rights or any other rights, including without limitation, copyright, trademark, patent, trade secret, privacy, publicity or confidentiality obligations; or (ii) otherwise violates any applicable state or federal law.</p>

              <p>To the maximum extent permitted by law, you indemnify and agree to keep indemnified challenge Entities at all times from and against any liability, claims, demands, losses, damages, costs and expenses resulting from any act, default or omission of the entrant and/or a breach of any warranty set forth herein. To the maximum extent permitted by law, you agree to defend, indemnify and hold harmless the challenge Entities from and against any and all claims, actions, suits or proceedings, as well as any and all losses, liabilities, damages, costs and expenses (including reasonable attorneys fees) arising out of or accruing from: (a) your Submission or other material uploaded or otherwise provided by you that infringes any copyright, trademark, trade secret, trade dress, patent or other intellectual property right of any person or entity, or defames any person or violates their rights of publicity or privacy; (b) any misrepresentation made by you in connection with the challenge; (c) any non-compliance by you with these Rules; (d) claims brought by persons or entities other than the parties to these Rules arising from or related to your involvement with the challenge; and (e) your acceptance, possession, misuse or use of any Prize, or your participation in the challenge and any challenge-related activity.</p>

              <p>You hereby release organizers from any liability associated with: (a) any malfunction or other problem with the challenge Website; (b) any error in the collection, processing, or retention of any Submission; or (c) any typographical or other error in the printing, offering or announcement of any Prize or winners.</p>
            </div>
          </div>
        </div>
      </section>
    
    <!--
      <section class="module-medium" style="padding:20px 0px 30px" id='framework'>
        <div class="container">
          <div class="row" style="text-align: justify">
            <div class="col-sm-12 font-nat" style="font-size: 13pt">            
              <h3 class="module-title font-alt" style="margin: 0px; padding: 30px 0 20px">Competition Framework</h3>
              <p> The Participant has requested permission to use the dataset as compiled by Texas A&amp;M University, Peking University, and University of Chinese Academy of Sciences. In exchange for such permission, Participant hereby agrees to the following terms and conditions:</p>
                <ul>
                  <li><p>Texas A&amp;M University, Peking University, and University of Chinese Academy of Sciences make no representations or warranties regarding the Dataset, including but not limited to warranties of non-infringement or fitness for a particular purpose.</p></li>
                  <li><p>Pre-trained models are allowed in the competition.</p></li>
                  <li><p>Participants are not restricted to train their algorithms on the provided training set. Collecting and training on additional data is encouraged.</p></li>
                </ul>
            </div>
          </div>
        </div>
      </section>
    -->

      <!-- <section class="module-medium" style="padding:20px 0px 30px" id='FAQ'>
        <div class="container">
          <div class="row" style="text-align: justify">
            <div class="col-sm-12 font-nat" style="font-size: 13pt">            
              <h3 class="module-title font-alt" style="margin: 0px; padding: 30px 0 20px">Frequently asked questions</h3>
              <p><ul><li> FAQ list will be continuously maintained and updated based on incoming questions: <a href='https://docs.google.com/document/d/11iWpmS0k-ohp58APZlpOdU0w54Spn1MCeRzuZ4PDL5w/edit' style="color: #337ab7; text-decoration: underline">FAQ list</a></li></ul></p>
              

            </div>
          </div>
        </div>
      </section> -->

      <a id="ddfooterLink" href="footer.html">Footer</a>
      <div class="scroll-up"><a href="#totop"><i class="fa fa-angle-double-up"></i></a></div>
    </main>
    <!--  
    JavaScripts
    =============================================
    -->
    <script src="assets/lib/jquery/dist/jquery.js"></script>
    <script src="assets/lib/bootstrap/dist/js/bootstrap.min.js"></script>
    <script src="assets/lib/wow/dist/wow.js"></script>
    <script src="assets/lib/jquery.mb.ytplayer/dist/jquery.mb.YTPlayer.js"></script>
    <script src="assets/lib/isotope/dist/isotope.pkgd.js"></script>
    <script src="assets/lib/imagesloaded/imagesloaded.pkgd.js"></script>
    <!-- <script src="assets/lib/flexslider/jquery.flexslider.js"></script> -->
    <script src="assets/lib/owl.carousel/dist/owl.carousel.min.js"></script>
    <!-- <script src="assets/lib/smoothscroll.js"></script> -->
    <script src="assets/lib/magnific-popup/dist/jquery.magnific-popup.js"></script>
    <script src="assets/lib/simple-text-rotator/jquery.simple-text-rotator.min.js"></script>
    <script src="assets/js/plugins.js"></script>
    <script src="assets/js/main.js"></script>
    <script src="assets/js/ddmenu.js" type="text/javascript"></script>
    <script src="assets/js/ddfooter.js" type="text/javascript"></script>
</body>
</html>